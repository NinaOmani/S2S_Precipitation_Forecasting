{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinaOmani/S2S_Precipitation_Forecasting/blob/main/A_Hindcast_Reanalysis_Predictors_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hindcast and Reanalysis Predictor Preparation\n",
        "\n",
        "**Author**: Nina Omani  \n",
        "**Created**: 01-28-2025\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Description\n",
        "\n",
        "This notebook prepares predictors for seasonal precipitation forecasting using:\n",
        "\n",
        "- **ECMWF** Hindcast Q850_based Weather Types (WT)   \n",
        "- **ERA5** Reanalysis Q850_based Weather Types (WT)\n",
        "- **ECMWF** Hindcast Climate Variables    \n",
        "- **ERA5** Reanalysis Climate Variables  \n",
        "- **PRISM** Precipitation Observations  \n",
        "- **ENSO** Indices (MEI, SOI, dSST3.4)  \n",
        "- **Antecedent Precipitation** Calculations  \n",
        "\n",
        "It merges and processes these datasets for AZ and NM regions across monsoon seasons, producing a final merged dataset of relevant predictors for precipitation forecasting.\n",
        "\n",
        "---\n",
        "\n",
        "## Workflow Overview\n",
        "\n",
        "1. Load and preprocess WT and PRISM precipitation data  \n",
        "2. Merge with ENSO indices  \n",
        "3. Aggregate by season and lead month  \n",
        "4. Process ERA and ECMWF climate variables  \n",
        "5. Calculate antecedent precipitation  \n",
        "6. Merge all into a unified predictor dataset\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NCkB_R8nP4tR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pECcQzAX4mfo",
        "outputId": "d61af873-ec17-4fa5-d103-8e032236032a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn scipy matplotlib\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BNJ24BBM5Mzc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.spatial.distance import mahalanobis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjlGFKYdOgO9",
        "outputId": "3341a46e-2d78-4645-a2c2-a677a300dcd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O_ucK_DHT392"
      },
      "outputs": [],
      "source": [
        "# Define forecast and historic years\n",
        "hindcast_years = list(range(1981, 2023))  # 1981 to 2023 (last update: Feb 2024)\n",
        "historic_years = list(range(1985, 2022))  # 1985 to 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tOnEwvmM8wqb"
      },
      "outputs": [],
      "source": [
        "model_dir = \"ECMWF\"\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z9cs4hKaSzdc"
      },
      "outputs": [],
      "source": [
        "# Define a region\n",
        "regions = [\"AZ_West\", \"AZ_East\", \"NM_South\", \"NM_North\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROw6pIc5cNbB"
      },
      "source": [
        "### **PRISM + ECMWF WT + ERA WT**\n",
        "\n",
        "The file `paired_PRISM_ECMWF_WT_1981_2023.csv` contains combined Weather Type data (from ECMWF and ERA reanalysis) and regional PRISM precipitation data.\n",
        "\n",
        "**Source Notebook**: `WT_File_gather.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxkJtFPtAhFL",
        "outputId": "62b16ee0-09ed-4b03-fcda-d23eed8f1052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Gjwgcr5HWnBCxUfhHFfcKDlmwN_ti81I\n",
            "To: /content/paired_PRISM_ECMWF_WT_1981_2023.csv\n",
            "100%|██████████| 301k/301k [00:00<00:00, 65.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df_prism_wt = pd.read_csv(\"paired_PRISM_ECMWF_WT_1981_2023.csv\")\n",
        "df_prism_wt[\"Month\"] = pd.to_numeric(df_prism_wt[\"Month\"], errors='coerce')\n",
        "df_prism_wt[\"Year\"] = pd.to_numeric(df_prism_wt[\"Year\"], errors='coerce').astype(\"Int64\")\n",
        "df_prism_wt.to_csv(\"df_prism_wt.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tql7RdVdcVnC"
      },
      "source": [
        "📈 **ENSO Data**  \n",
        "`ENSO_data.csv` contains ENSO climate indices used in the analysis.  \n",
        "📂 **Source Notebook**: `Climate_indices.ipynb` in R\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu48NozHZT2X",
        "outputId": "429ca9dd-fcff-4562-c23f-75ad7d6170e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yBUuG60_7_nrSUsLdxnRb2c2HyuHByUd\n",
            "To: /content/ENSO_data.csv\n",
            "100%|██████████| 44.5k/44.5k [00:00<00:00, 36.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "enso = pd.read_csv(\"ENSO_data.csv\")\n",
        "\n",
        "month_map = {\n",
        "    \"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\": 6,\n",
        "    \"Jul\": 7, \"Aug\": 8, \"Sep\": 9, \"Oct\": 10, \"Nov\": 11, \"Dec\": 12\n",
        "}\n",
        "\n",
        "if 'Date' in enso.columns:\n",
        "    enso.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "enso[\"Month\"] = enso[\"Month\"].map(month_map)\n",
        "enso[\"Year\"] = pd.to_numeric(enso[\"Year\"], errors='coerce').astype(\"Int64\")\n",
        "enso[\"Month\"] = pd.to_numeric(enso[\"Month\"], errors='coerce').astype(\"Int64\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine ENSO and WT**"
      ],
      "metadata": {
        "id": "hNkP-F-Yxx4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XDh50M1nRoKG"
      },
      "outputs": [],
      "source": [
        "# Convert Year and Month columns to integer type in both datasets for consistency\n",
        "df2= enso.copy()\n",
        "df1= df_prism_wt.copy()\n",
        "\n",
        "enso_WT = pd.merge(df1, df2, on=['Year', 'Month'], how='outer')\n",
        "enso_WT = enso_WT.sort_values(by=['Year', 'lead_month', 'Month', 'region'], ascending=[True, True, True, True])\n",
        "enso_WT = enso_WT[enso_WT['Year'] > 1980]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process ENSO Features**"
      ],
      "metadata": {
        "id": "SR3dYU8Ix52X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xsmausg6qk-",
        "outputId": "b3d7611e-93c0-460d-a8c4-186341a035cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ENSO Features successfully saved to: ENSO_Features.csv\n"
          ]
        }
      ],
      "source": [
        "enso_features = enso_WT.copy()\n",
        "lead_months = [4, 5, 6, 7, 8]\n",
        "new_cols = []\n",
        "enso_predictors = [\"SOI\", \"MEI\", \"dSST3.4\"]\n",
        "\n",
        "for pred in enso_predictors:\n",
        "    # Define column names for observed and forecast values\n",
        "    obs_col = f\"ant_{pred}_obs\"\n",
        "    for_col = f\"ant_{pred}_for\"\n",
        "\n",
        "    # Assign observed values\n",
        "    enso_features[obs_col] = enso_features[pred]\n",
        "\n",
        "    for lead_month in lead_months:\n",
        "        antecedent_month = lead_month - 1\n",
        "\n",
        "        for year in enso_features[\"Year\"].unique():\n",
        "            antecedent_value = enso_features.loc[\n",
        "                (enso_features[\"Year\"] == year) & (enso_features[\"Month\"] == antecedent_month), pred\n",
        "            ].values\n",
        "\n",
        "            if len(antecedent_value) > 0:  # Ensure there's a valid antecedent value\n",
        "                enso_features.loc[\n",
        "                    (enso_features[\"Year\"] == year) &\n",
        "                    (enso_features[\"Month\"].isin([6, 7, 8, 9, 10])) &\n",
        "                    (enso_features[\"lead_month\"] == lead_month),\n",
        "                    for_col\n",
        "                ] = antecedent_value[0]\n",
        "\n",
        "    new_cols.extend([obs_col, for_col])\n",
        "\n",
        "# Save the ENSO features\n",
        "enso_features.to_csv(\"ENSO_Features.csv\", index=False)\n",
        "print(f\"✅ ENSO Features successfully saved to: ENSO_Features.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Seasonal ENSO features**"
      ],
      "metadata": {
        "id": "a07YMfc90IF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gqPd8eBAPpcc"
      },
      "outputs": [],
      "source": [
        "df= enso_features.copy()\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "# Define seasons\n",
        "season_definitions = {\n",
        "    \"Jun\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"] == 6),\n",
        "    \"Jul\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"] == 7),\n",
        "    \"Aug\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 8),\n",
        "    \"Sep\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 9),\n",
        "    \"Oct\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 10)}\n",
        "\n",
        "final_data = pd.DataFrame()\n",
        "\n",
        "rename_dict = {\n",
        "    \"areal_mean_precipitation_mm_day\": \"avgPCP_mmDay\"\n",
        "}\n",
        "\n",
        "final_data = pd.concat([\n",
        "    df[condition].assign(season=season).rename(columns={k: v for k, v in rename_dict.items() if k in df.columns})\n",
        "    for season, condition in season_definitions.items()\n",
        "], ignore_index=True)\n",
        "\n",
        "# Sort final dataset by region, season, lead_month, and year\n",
        "final_data.sort_values(by=[\"region\", \"season\", \"lead_month\", \"year\"], inplace=True)\n",
        "\n",
        "agg_ENSO_PRISM = final_data.copy()\n",
        "\n",
        "output_file = \"agg_ENSO_PRISM.csv\"\n",
        "agg_ENSO_PRISM.to_csv(output_file, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPC4LNdhJHlg"
      },
      "source": [
        "**Load ERA5 and seasonal ECMWF climate variables: TPRATE, TCLW, TCWV, Q850**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siO142xPONKG",
        "outputId": "29b9909d-6070-426e-e54f-84783c2ce214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PRISM_CLIMAVAR successfully created with all variables.\n"
          ]
        }
      ],
      "source": [
        "clim_vars = [\"q\", \"tclw\", \"tcwv\", \"tprate\"]\n",
        "\n",
        "# forecast ECMWF variables\n",
        "forecast_data = pd.read_csv(\"5_combined_forecast_variables.csv\")\n",
        "forecast_data = forecast_data.rename(columns={\"init_Month\": \"lead_month\", \"Region\": \"region\"})\n",
        "forecast_data[\"lead_month\"] = pd.to_numeric(forecast_data[\"lead_month\"], errors='coerce')\n",
        "\n",
        "#########################################################################################################\n",
        "# ERA reanalysis climate variables\n",
        "\n",
        "df_prism_wt[\"lead_month\"] = pd.to_numeric(df_prism_wt[\"lead_month\"], errors='coerce')\n",
        "\n",
        "PRISM_CLIMAVAR = df_prism_wt.copy()\n",
        "\n",
        "# Process each predictor dynamically\n",
        "for var in clim_vars:\n",
        "    file_path = f\"ERA_{var}_regional.csv\"\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        era_data = pd.read_csv(file_path)\n",
        "        era_data = era_data.rename(columns={\"Region\": \"region\"})\n",
        "\n",
        "        # Rename \"tp\" to \"tprate\" if it exists\n",
        "        if \"tp\" in era_data.columns:\n",
        "            era_data[\"tp\"] = era_data[\"tp\"] / 86400  # Convert from total precipitation (m) to rate (m/s)\n",
        "            era_data.rename(columns={\"tp\": \"tprate\"}, inplace=True)\n",
        "\n",
        "        # Merge ERA data with weather data for observations\n",
        "        PRISM_CLIMAVAR = PRISM_CLIMAVAR.merge(\n",
        "            era_data[[\"Year\", \"Month\", \"region\", var]],\n",
        "            on=[\"Year\", \"Month\", \"region\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Rename observation columns dynamically\n",
        "        var_obs = f\"{var}_obs\"\n",
        "        PRISM_CLIMAVAR.rename(columns={var: var_obs}, inplace=True)\n",
        "\n",
        "        # Merge with forecast data\n",
        "        PRISM_CLIMAVAR = PRISM_CLIMAVAR.merge(\n",
        "            forecast_data[[\"Year\", \"Month\", \"region\", \"lead_month\", var]],\n",
        "            on=[\"Year\", \"Month\", \"region\", \"lead_month\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Rename forecast columns dynamically\n",
        "        var_for = f\"{var}_for\"\n",
        "        PRISM_CLIMAVAR.rename(columns={var: var_for}, inplace=True)\n",
        "    else:\n",
        "        print(f\"Warning: File not found: {file_path}\")\n",
        "\n",
        "print(\"✅ PRISM_CLIMAVAR successfully created with all variables.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Seasonal climate features**"
      ],
      "metadata": {
        "id": "s6dAEwU13Nd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8t4RX2-iKZcj"
      },
      "outputs": [],
      "source": [
        "# Copy and clean column names\n",
        "df = PRISM_CLIMAVAR.copy()\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "# Define season filtering conditions\n",
        "'''\n",
        "season_definitions = {\n",
        "    \"Jun\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"] == 6),\n",
        "    \"Jul\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"] == 7),\n",
        "    \"Aug\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 8),\n",
        "    \"Sep\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 9),\n",
        "    \"Oct\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 10),\n",
        "    \"JJASO\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7, 8, 9, 10])),\n",
        "    \"JJA\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7, 8])),\n",
        "    \"JJ\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7])),\n",
        "    \"JASO\": (df[\"lead_month\"] == 7) & (df[\"month\"].isin([7, 8, 9, 10])),\n",
        "    \"JAS\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8, 9])),\n",
        "    \"JA\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8])),\n",
        "    \"ASO\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([8, 9, 10])),\n",
        "    \"AS\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([8, 9])),\n",
        "    \"SO\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([9, 10])),\n",
        "}\n",
        "'''\n",
        "# Define season filtering conditions (for October forecast lead month 4 was eliminated)\n",
        "season_definitions = {\n",
        "    \"Jun\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"] == 6),\n",
        "    \"Jul\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"] == 7),\n",
        "    \"Aug\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 8),\n",
        "    \"Sep\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 9),\n",
        "    \"Oct\": (df[\"lead_month\"].isin([5, 6, 7, 8])) & (df[\"month\"] == 10),\n",
        "    \"JJASO\": (df[\"lead_month\"].isin([5, 6])) & (df[\"month\"].isin([6, 7, 8, 9, 10])),\n",
        "    \"JJA\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7, 8])),\n",
        "    \"JJ\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7])),\n",
        "    \"JASO\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8, 9, 10])),\n",
        "    \"JAS\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8, 9])),\n",
        "    \"JA\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8])),\n",
        "    \"ASO\": (df[\"lead_month\"].isin([5, 6, 7, 8])) & (df[\"month\"].isin([8, 9, 10])),\n",
        "    \"AS\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([8, 9])),\n",
        "    \"SO\": (df[\"lead_month\"].isin([5, 6, 7, 8])) & (df[\"month\"].isin([9, 10])),\n",
        "}\n",
        "\n",
        "# Create final container for all aggregated results\n",
        "final_data = pd.DataFrame()\n",
        "\n",
        "# Loop over all seasons\n",
        "for season, condition in season_definitions.items():\n",
        "    season_data = df[condition].copy()\n",
        "    season_data[\"season\"] = season\n",
        "\n",
        "    # Dynamically build aggregation dictionary\n",
        "    agg_dict = {}\n",
        "    for var in clim_vars:\n",
        "        for suffix in [\"_for\", \"_obs\"]:\n",
        "            col = f\"{var}{suffix}\"\n",
        "            if col in season_data.columns:\n",
        "                agg_dict[col] = \"sum\"\n",
        "    if \"areal_mean_precipitation_mm_day\" in season_data.columns:\n",
        "        agg_dict[\"areal_mean_precipitation_mm_day\"] = \"mean\"\n",
        "\n",
        "    # Group and aggregate\n",
        "    aggregated = season_data.groupby([\"year\", \"region\", \"season\", \"lead_month\"], as_index=False).agg(agg_dict)\n",
        "\n",
        "    # Rename columns after aggregation\n",
        "    rename_dict = {\n",
        "        col: f\"sum{col}\" for col in agg_dict if col.endswith(\"_for\") or col.endswith(\"_obs\")\n",
        "    }\n",
        "    if \"areal_mean_precipitation_mm_day\" in agg_dict:\n",
        "        rename_dict[\"areal_mean_precipitation_mm_day\"] = \"avgPCP_mmDay\"\n",
        "\n",
        "    aggregated.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "    # Append to final data\n",
        "    final_data = pd.concat([final_data, aggregated], ignore_index=True)\n",
        "\n",
        "# Standardize _for and _obs variables (Z-score by region & season)\n",
        "for col in [c for c in final_data.columns if c.endswith(\"_for\") or c.endswith(\"_obs\")]:\n",
        "    final_data[f\"sd_{col}\"] = final_data.groupby([\"region\", \"season\"])[col].transform(\n",
        "        lambda x: (x - x.mean()) / x.std() if x.std() != 0 else 0\n",
        "    )\n",
        "\n",
        "# Final sort\n",
        "agg_PRISM_CLIMAVAR = final_data.sort_values(by=[\"region\", \"season\", \"lead_month\", \"year\"]).copy()\n",
        "\n",
        "# Save to CSV\n",
        "agg_PRISM_CLIMAVAR.to_csv(f\"agg_PRISM_CLIMAVAR_{'_'.join(clim_vars)}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knj8Fc70ljrT"
      },
      "source": [
        "## **Observed Antecedent Precipitation (era5)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JmmegBhHizfs"
      },
      "outputs": [],
      "source": [
        "# Load the ERA5 reanalysis data\n",
        "era5_file_path = \"ERA_tprate_regional.csv\"\n",
        "era5_tprate = pd.read_csv(era5_file_path)\n",
        "era5_tprate[\"tp\"] = era5_tprate[\"tp\"] / 86400  # Convert from total precipitation (m) to rate (m/s)\n",
        "# Load the forecasted precipitation data\n",
        "forecast_var_path = \"5_combined_forecast_variables.csv\"  # Ensure correct file path\n",
        "forecast_data = pd.read_csv(forecast_var_path)\n",
        "\n",
        "# Convert 'time' column in ERA5 data to datetime format for proper indexing\n",
        "era5_tprate['time'] = pd.to_datetime(era5_tprate['time'])\n",
        "\n",
        "# Define initialization months and corresponding target months\n",
        "init_months = [4, 5, 6, 7, 8]  # April to August\n",
        "target_months_by_init = {\n",
        "    4: [6, 7, 8, 9, 10],  # April init → June to October\n",
        "    5: [6,7, 8, 9, 10],     # May init → July to October\n",
        "    6: [6,7, 8, 9, 10],        # June init → August to October\n",
        "    7: [7, 8, 9, 10],           # July init → September to October\n",
        "    8: [8, 9, 10]               # August init → October\n",
        "}\n",
        "\n",
        "# Initialize list to store historical antecedent precipitation\n",
        "historical_antecedent_data = []\n",
        "\n",
        "# Get unique years and regions from ERA5 data\n",
        "years = historic_years\n",
        "regions = era5_tprate[\"Region\"].unique()\n",
        "\n",
        "# Loop through each year and initialization month\n",
        "for year in years:\n",
        "    for init_month in init_months:\n",
        "        # Get ERA5 data for this year\n",
        "        era5_subset = era5_tprate[era5_tprate[\"Year\"] == year]\n",
        "\n",
        "        for target_month in target_months_by_init[init_month]:  # Only valid target months\n",
        "            for region in regions:\n",
        "                # Filter region-specific data\n",
        "                region_data = era5_subset[era5_subset[\"Region\"] == region]\n",
        "\n",
        "                # Select antecedent months\n",
        "                if init_month == 4:  # April Init → Jan, Feb, Mar\n",
        "                    months = [1, 2, 3]\n",
        "                elif init_month == 5:  # May Init → Feb, Mar, Apr\n",
        "                    months = [2, 3, 4]\n",
        "                elif init_month == 6:  # June Init → Mar, Apr, May\n",
        "                    months = [3, 4, 5]\n",
        "                elif init_month == 7:  # July Init → Apr, May, June\n",
        "                    months = [4, 5, 6]\n",
        "                elif init_month == 8:  # August Init → May, June, July\n",
        "                    months = [5, 6, 7]\n",
        "                else:\n",
        "                    continue  # Skip invalid cases\n",
        "\n",
        "                # Extract precipitation values for the selected months\n",
        "                precip_values = region_data[region_data[\"Month\"].isin(months)][\"tp\"].values\n",
        "\n",
        "                # Ensure we have enough data\n",
        "                if len(precip_values) < 3:\n",
        "                    antecedent_3m, antecedent_2m, antecedent_1m = np.nan, np.nan, np.nan\n",
        "                else:\n",
        "                    antecedent_3m = np.mean(precip_values)  # Average over 3 months\n",
        "                    antecedent_2m = np.mean(precip_values[1:])  # Average over last 2 months\n",
        "                    antecedent_1m = precip_values[-1]  # Last month's value\n",
        "\n",
        "                # Store results\n",
        "                historical_antecedent_data.append({\n",
        "                    \"Year\": year,\n",
        "                    \"Init_Month\": init_month,\n",
        "                    \"Target_Month\": target_month,\n",
        "                    \"Region\": region,\n",
        "                    \"Antecedent_3M\": antecedent_3m,\n",
        "                    \"Antecedent_2M\": antecedent_2m,\n",
        "                    \"Antecedent_1M\": antecedent_1m\n",
        "                })\n",
        "\n",
        "# Convert to DataFrame\n",
        "historic_antecedent_precip = pd.DataFrame(historical_antecedent_data)\n",
        "\n",
        "# Save the dataset\n",
        "historic_antecedent_precip.to_csv(\"historic_antecedent_precip.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jxi5Zv7lozt"
      },
      "source": [
        "## **Hindcast Antecedent Precipitation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKNdna7Eizju",
        "outputId": "23078555-3a5f-442d-b5f6-7afb855e9aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Missing Antecedent_1M values filled with ERA5 for same Target and Init Month cases!\n"
          ]
        }
      ],
      "source": [
        "# Convert 'time' column in ERA5 data to datetime format for proper indexing\n",
        "era5_tprate['time'] = pd.to_datetime(era5_tprate['time'])\n",
        "era5_tprate[\"tp\"] = era5_tprate[\"tp\"]   # Convert from total precipitation (m) to rate (m/s)\n",
        "# Define initialization months and corresponding target months\n",
        "init_months = [4, 5, 6, 7, 8]  # April to August\n",
        "target_months_by_init = {\n",
        "    4: [6, 7, 8, 9,10],  # April init → June to October\n",
        "    5: [6,7, 8, 9, 10],  # May init → June to October\n",
        "    6: [6,7, 8, 9, 10],  # June init →cf June to October\n",
        "    7: [7, 8, 9, 10],  # July init → July to October\n",
        "    8: [8, 9, 10]  # August init → August to October\n",
        "}\n",
        "\n",
        "# Get unique years and regions from ERA5 data\n",
        "hindcast_years = forecast_data[\"Year\"].unique()\n",
        "regions = era5_tprate[\"Region\"].unique()\n",
        "\n",
        "# Initialize a list to store antecedent precipitation data\n",
        "antecedent_precip_data = []\n",
        "\n",
        "# Loop through each year in the hindcast dataset\n",
        "for year in hindcast_years:\n",
        "    for init_month in init_months:\n",
        "        if init_month not in target_months_by_init:\n",
        "            continue  # Skip if no valid target months for this init month\n",
        "\n",
        "        for target_month in target_months_by_init[init_month]:  # Only valid target months\n",
        "            for region in regions:\n",
        "                # Get forecasted data for this region, initialized in the given month\n",
        "                forecast_subset = forecast_data[\n",
        "                    (forecast_data[\"init_Month\"] == init_month) &\n",
        "                    (forecast_data[\"Year\"] == year) &\n",
        "                    (forecast_data[\"Region\"] == region)\n",
        "                ]\n",
        "\n",
        "                # Get ERA5 data for this region for the given year\n",
        "                era5_subset = era5_tprate[\n",
        "                    (era5_tprate[\"Year\"] == year) &\n",
        "                    (era5_tprate[\"Region\"] == region)\n",
        "                ]\n",
        "\n",
        "                # Identify antecedent months\n",
        "                antecedent_3m_months = [target_month - 3, target_month - 2, target_month - 1]\n",
        "                antecedent_2m_months = [target_month - 2, target_month - 1]\n",
        "                antecedent_1m_month = target_month - 1  # Usually from forecast\n",
        "\n",
        "                # Get precipitation values for each case\n",
        "                precip_values_3m = []\n",
        "                precip_values_2m = []\n",
        "                precip_value_1m = np.nan\n",
        "\n",
        "                for month in antecedent_3m_months:\n",
        "                    if month < init_month:  # Use ERA5 for months before initialization\n",
        "                        value = era5_subset.loc[era5_subset[\"Month\"] == month, \"tp\"]\n",
        "                    else:  # Use Forecast for months after initialization\n",
        "                        value = forecast_subset.loc[forecast_subset[\"Month\"] == month, \"tprate\"]\n",
        "\n",
        "                    if not value.empty:\n",
        "                        precip_values_3m.append(value.values[0])\n",
        "\n",
        "                for month in antecedent_2m_months:\n",
        "                    if month < init_month:  # Use ERA5 for months before initialization\n",
        "                        value = era5_subset.loc[era5_subset[\"Month\"] == month, \"tp\"]\n",
        "                    else:  # Use Forecast for months after initialization\n",
        "                        value = forecast_subset.loc[forecast_subset[\"Month\"] == month, \"tprate\"]\n",
        "\n",
        "                    if not value.empty:\n",
        "                        precip_values_2m.append(value.values[0])\n",
        "\n",
        "                # Assign 1-month antecedent value (default from forecast)\n",
        "                value_1m = forecast_subset.loc[forecast_subset[\"Month\"] == antecedent_1m_month, \"tprate\"]\n",
        "                if not value_1m.empty:\n",
        "                    precip_value_1m = value_1m.values[0]\n",
        "\n",
        "                # **NEW FIX**: If `Target Month == Init Month`, use ERA5 from `Target Month - 1`\n",
        "                if target_month == init_month:\n",
        "                    era5_value_1m = era5_subset.loc[era5_subset[\"Month\"] == antecedent_1m_month, \"tp\"]\n",
        "                    if not era5_value_1m.empty:\n",
        "                        precip_value_1m = era5_value_1m.values[0]  # Fill from ERA5\n",
        "\n",
        "                # Compute means\n",
        "                antecedent_3m = np.nan if len(precip_values_3m) < 3 else np.mean(precip_values_3m)\n",
        "                antecedent_2m = np.nan if len(precip_values_2m) < 2 else np.mean(precip_values_2m)\n",
        "                antecedent_1m = precip_value_1m  # Direct value\n",
        "\n",
        "                # Store the results\n",
        "                antecedent_precip_data.append({\n",
        "                    \"Year\": year,\n",
        "                    \"Init_Month\": init_month,\n",
        "                    \"Target_Month\": target_month,\n",
        "                    \"Region\": region,\n",
        "                    \"Antecedent_3M\": antecedent_3m,\n",
        "                    \"Antecedent_2M\": antecedent_2m,\n",
        "                    \"Antecedent_1M\": antecedent_1m\n",
        "                })\n",
        "\n",
        "# Convert to DataFrame\n",
        "hindcast_antecedent_precip = pd.DataFrame(antecedent_precip_data)\n",
        "\n",
        "# Save the dataset\n",
        "hindcast_antecedent_precip.to_csv(\"hindcast_antecedent_precip.csv\", index=False)\n",
        "\n",
        "print(\"✅ Missing Antecedent_1M values filled with ERA5 for same Target and Init Month cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW6oeIJ3lz0L"
      },
      "source": [
        "## **Combine observed and hindcast antecedent precipitation in one data file.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3vi0_fmizry",
        "outputId": "cd8d235d-05b0-4079-8ce0-eb683a53a041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined antecedent precipitation dataset saved as: combined_antecedent_precip.csv\n"
          ]
        }
      ],
      "source": [
        "# Merge the hindcast and historic antecedent data on Year, Init_Month, Target_Month, and Region\n",
        "# Perform a full outer merge to keep all data\n",
        "combined_antecedent_precip = hindcast_antecedent_precip.merge(\n",
        "    historic_antecedent_precip,\n",
        "    on=[\"Year\", \"Init_Month\", \"Target_Month\", \"Region\"],\n",
        "    how=\"outer\",  # Keep all data from both datasets\n",
        "    suffixes=(\"_for\", \"_obs\")  # Ensuring correct naming of columns\n",
        ")\n",
        "\n",
        "file_path = \"combined_antecedent_precip.csv\"\n",
        "combined_antecedent_precip.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"✅ Combined antecedent precipitation dataset saved as: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TtckeV7614Fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "combined_antecedent_precip.rename(columns={'Target_Month': 'Month'}, inplace=True)\n",
        "combined_antecedent_precip.rename(columns={'Init_Month': 'lead_month'}, inplace=True)\n",
        "\n",
        "# Merge df_prism_wt with combined_antecedent_precip on 'Year', 'Month', and 'region'\n",
        "antecedent_precip_prism = df_prism_wt.merge(\n",
        "    combined_antecedent_precip,\n",
        "    left_on=['lead_month', 'Year', 'Month', 'region'],\n",
        "    right_on=['lead_month','Year', 'Month', 'Region'],\n",
        "    how='left',\n",
        "    suffixes=('', '_del')\n",
        ").drop(columns=['Region'])\n",
        "\n",
        "\n",
        "# Remove columns that have the suffix '_del'\n",
        "columns_to_keep = [col for col in antecedent_precip_prism.columns if not col.endswith('_del')]\n",
        "antecedent_precip_prism = antecedent_precip_prism[columns_to_keep]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BYTso0Tc4oIX"
      },
      "outputs": [],
      "source": [
        "df = antecedent_precip_prism.copy()\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "# Define seasons\n",
        "season_definitions = {\n",
        "    \"Jun\": df[\"lead_month\"].isin([4, 5, 6]) & (df[\"month\"] == 6),\n",
        "    \"Jul\": df[\"lead_month\"].isin([4, 5, 6, 7]) & (df[\"month\"] == 7),\n",
        "    \"Aug\": df[\"lead_month\"].isin([4, 5, 6, 7, 8]) & (df[\"month\"] == 8),\n",
        "    \"Sep\": df[\"lead_month\"].isin([4, 5, 6, 7, 8]) & (df[\"month\"] == 9),\n",
        "    \"Oct\": df[\"lead_month\"].isin([4, 5, 6, 7, 8]) & (df[\"month\"] == 10)\n",
        "}\n",
        "final_data = pd.DataFrame()\n",
        "# Rename mapping\n",
        "rename_dict = {\n",
        "    \"areal_mean_precipitation_mm_day\": \"avgPCP_mmDay\"\n",
        "}\n",
        "\n",
        "# Process seasons, rename, and concatenate\n",
        "final_data = pd.concat([\n",
        "    df[condition].assign(season=season).rename(columns={k: v for k, v in rename_dict.items() if k in df.columns})\n",
        "    for season, condition in season_definitions.items()\n",
        "], ignore_index=True)\n",
        "\n",
        "\n",
        "# Sort dataset\n",
        "antecedent_precip_prism_df = final_data.sort_values(by=[\"region\", \"season\", \"lead_month\", \"year\"]).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TmfXMwZqGedS"
      },
      "outputs": [],
      "source": [
        "df = df_prism_wt.copy()\n",
        "\n",
        "# Convert column names to lowercase\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "# Define seasons\n",
        "season_definitions = {\n",
        "    \"Jun\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"] == 6),\n",
        "    \"Jul\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"] == 7),\n",
        "    \"Aug\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 8),\n",
        "    \"Sep\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 9),\n",
        "    \"Oct\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"] == 10),\n",
        "    \"JJASO\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7, 8, 9, 10])),\n",
        "    \"JJA\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7, 8])),\n",
        "    \"JJ\": (df[\"lead_month\"].isin([4, 5, 6])) & (df[\"month\"].isin([6, 7])),\n",
        "    \"JASO\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8, 9, 10])),\n",
        "    \"JAS\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8, 9])),\n",
        "    \"JA\": (df[\"lead_month\"].isin([4, 5, 6, 7])) & (df[\"month\"].isin([7, 8])),\n",
        "    \"ASO\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([8, 9, 10])),\n",
        "    \"AS\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([8, 9])),\n",
        "    \"SO\": (df[\"lead_month\"].isin([4, 5, 6, 7, 8])) & (df[\"month\"].isin([9, 10])),\n",
        "}\n",
        "\n",
        "final_data = pd.DataFrame()\n",
        "\n",
        "for season, condition in season_definitions.items():\n",
        "    # Filter dataset for current season\n",
        "    season_data = df[condition].copy()\n",
        "\n",
        "    season_data[\"season\"] = season\n",
        "\n",
        "    # Aggregate data using groupby\n",
        "    aggregated = season_data.groupby([\"year\", \"region\", \"season\", \"lead_month\"], as_index=False).agg({\n",
        "        \"dry.for\": \"sum\",\n",
        "        \"normal.for\": \"sum\",\n",
        "        \"monsoon.for\": \"sum\",\n",
        "        \"dry.obs\": \"sum\",\n",
        "        \"normal.obs\": \"sum\",\n",
        "        \"monsoon.obs\": \"sum\",\n",
        "        \"areal_mean_precipitation_mm_day\": \"mean\"\n",
        "    })\n",
        "\n",
        "    # Fix multi-index column names (Flatten multi-index)\n",
        "    aggregated.columns = [\"_\".join(col).strip(\"_\") if isinstance(col, tuple) else col for col in aggregated.columns]\n",
        "\n",
        "    # Rename columns to match the required output format\n",
        "    aggregated.rename(columns={\n",
        "        \"dry.for\": \"sumDry.for\",\n",
        "        \"monsoon.for\": \"sumMonsoon.for\",\n",
        "        \"dry.obs\": \"sumDry.obs\",\n",
        "        \"monsoon.obs\": \"sumMonsoon.obs\",\n",
        "        \"areal_mean_precipitation_mm_day\": \"avgPCP_mmDay\",  # Mean precipitation\n",
        "}, inplace=True)\n",
        "\n",
        "    final_data = pd.concat([final_data, aggregated], ignore_index=True)\n",
        "\n",
        "# Sort final dataset by region, season, lead_month, and year\n",
        "final_data.sort_values(by=[\"region\", \"season\", \"lead_month\", \"year\"], inplace=True)\n",
        "\n",
        "agg_WT_PRISM = final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_o5Sg3KwZ4mg"
      },
      "outputs": [],
      "source": [
        "antecedent_precip_prism_WT = pd.DataFrame()\n",
        "antecedent_precip_prism_WT = agg_WT_PRISM.merge(\n",
        "    antecedent_precip_prism_df,\n",
        "    on=['lead_month', 'year', 'region', 'season'],\n",
        "    how='left',\n",
        "    suffixes=('', '_del')  # Adding suffix for duplicate column identification\n",
        ")\n",
        "\n",
        "# Remove columns that have the suffix '_del'\n",
        "columns_to_keep = [col for col in antecedent_precip_prism_WT.columns if not col.endswith('_del')]\n",
        "\n",
        "# Keep only the necessary columns\n",
        "antecedent_precip_prism_WT = antecedent_precip_prism_WT[columns_to_keep]\n",
        "antecedent_precip_prism_df.to_csv(\"antecedent_precip_prism_df.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg_WT_PRISM.to_csv(\"paired_allSeasons_ECMWF_PRISM_WT_1981_2023.csv\", index=False)"
      ],
      "metadata": {
        "id": "DxcDhLel_-jQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Predictors"
      ],
      "metadata": {
        "id": "xGQeHv7hJfoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FcQ-OloCip1I",
        "outputId": "1df6cf51-0ec9-4afa-9f7a-bdb2106eba91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Predictors.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# File paths\n",
        "base_path = \"\"\n",
        "file_names = [\n",
        "    \"paired_allSeasons_ECMWF_PRISM_WT_1981_2023.csv\",\n",
        "    \"antecedent_precip_prism_df.csv\",\n",
        "    \"agg_PRISM_CLIMAVAR_q_tclw_tcwv_tprate.csv\",\n",
        "    \"agg_ENSO_PRISM.csv\"\n",
        "]\n",
        "\n",
        "# Load files only if they exist\n",
        "dataframes = {}\n",
        "for file_name in file_names:\n",
        "    full_path = os.path.join(base_path, file_name)\n",
        "    if os.path.exists(full_path):\n",
        "        df_name = os.path.splitext(file_name)[0].replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "        dataframes[df_name] = pd.read_csv(full_path)\n",
        "\n",
        "# Start with the base dataset\n",
        "main_key = \"paired_allSeasons_ECMWF_PRISM_WT_1981_2023\"\n",
        "merged_df = dataframes[main_key]\n",
        "\n",
        "# Columns to exclude if they appear in any file\n",
        "excluded_cols = {\"yyyymm\", \"dry.obs\", \"normal.obs\", \"monsoon.obs\", \"month\", \"lead_date\",\n",
        "                 \"dry.for\", \"normal.for\", \"monsoon.for\", \"date\", \"avgPCP_mmDay\"}\n",
        "\n",
        "# Merge other datasets on ['region', 'season', 'year', 'lead_month']\n",
        "merge_keys = [\"region\", \"season\", \"year\", \"lead_month\"]\n",
        "for key, df in dataframes.items():\n",
        "    if key != main_key:\n",
        "        cols_to_use = [col for col in df.columns if col not in excluded_cols]\n",
        "        df_filtered = df[merge_keys + [col for col in cols_to_use if col not in merge_keys]]\n",
        "        merged_df = pd.merge(merged_df, df_filtered, on=merge_keys, how=\"left\")\n",
        "\n",
        "# Drop columns containing \"sd\"\n",
        "merged_df = merged_df.loc[:, ~merged_df.columns.str.contains(\"sd\")]\n",
        "# Rename _for and _obs to .for and .obs\n",
        "merged_df.columns = merged_df.columns.str.replace(\"_for\", \".for\").str.replace(\"_obs\", \".obs\")\n",
        "\n",
        "# Save the final cleaned merged dataset\n",
        "output_path = \"Predictors.csv\"\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "output_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill seasonal antecedent precipitation and ENSO predictors using the monthly values."
      ],
      "metadata": {
        "id": "VA2HBTYSOrWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "Predictors = pd.read_csv(\"Predictors.csv\")\n",
        "\n",
        "# Update column names to use _for and _obs instead of .for and .obs\n",
        "Predictors.columns = [col.replace(\".for\", \"_for\").replace(\".obs\", \"_obs\") for col in Predictors.columns]\n",
        "\n",
        "# Update the list of antecedent and ENSO-related columns\n",
        "antecedent_cols_extended = [\n",
        "    \"antecedent_3m_for\", \"antecedent_2m_for\", \"antecedent_1m_for\",\n",
        "    \"antecedent_3m_obs\", \"antecedent_2m_obs\", \"antecedent_1m_obs\",\n",
        "    \"ant_dsst3_4_for\", \"ant_mei_for\", \"ant_soi_for\",\n",
        "    \"ant_dsst3_4_obs\", \"ant_mei_obs\", \"ant_soi_obs\"\n",
        "]\n",
        "\n",
        "# Define season aggregation mapping\n",
        "agg_season_mapping = {\n",
        "    \"JJ\": \"Jun\", \"JA\": \"Jun\", \"AS\": \"Aug\", \"SO\": \"Sep\",\n",
        "    \"JJA\": \"Jun\", \"JAS\": \"Jul\", \"ASO\": \"Aug\", \"JASO\": \"Jul\", \"JJASO\": \"Jun\"\n",
        "}\n",
        "\n",
        "# Apply the fill logic\n",
        "for agg_season, base_month in agg_season_mapping.items():\n",
        "    for (region, year), group in Predictors.groupby([\"region\", \"year\"]):\n",
        "        agg_mask = (Predictors[\"region\"] == region) & \\\n",
        "                   (Predictors[\"year\"] == year) & \\\n",
        "                   (Predictors[\"season\"] == agg_season)\n",
        "        base_mask = (Predictors[\"region\"] == region) & \\\n",
        "                    (Predictors[\"year\"] == year) & \\\n",
        "                    (Predictors[\"season\"] == base_month)\n",
        "\n",
        "        if agg_mask.any() and base_mask.any():\n",
        "            for col in antecedent_cols_extended:\n",
        "                if col in Predictors.columns:\n",
        "                    if Predictors.loc[agg_mask, col].isna().all() and Predictors.loc[base_mask, col].notna().any():\n",
        "                        Predictors.loc[agg_mask, col] = Predictors.loc[base_mask, col].values[0]\n",
        "Predictors_filled = Predictors\n",
        "# Save the cleaned dataset\n",
        "Predictors_filled.to_csv(\"Predictors_filled.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "-0COO0WkGY9z"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNY0fYP6hen4rbQMcRX/Mi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}